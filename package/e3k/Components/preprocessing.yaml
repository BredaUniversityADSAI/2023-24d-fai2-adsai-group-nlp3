schema: http://azureml/sdk-2-0/CommandComponent.json
name: preprocessing_component
version: 1.0
type: command
description: Preprocessing component for tokenizing text data and encoding labels
environment:
  name: BlockD
  version: 1
code:
  local_path: ./
entryScript: preprocessing.py
inputs:
  train_data_path:
    type: uri_file
    description: Path to the training DataFrame pickle file
  val_data_path:
    type: uri_file
    description: Path to the validation DataFrame pickle file
  label_decoder_path:
    type: uri_file
    description: Path to the label decoder npz file
  output_path:
    type: uri_folder
    description: Base path to save the preprocessed data
  tokenizer_model:
    type: string
    default: roberta-base
    description: Model name for the tokenizer
  max_length:
    type: integer
    default: 128
    description: Maximum length for tokenized sequences
  batch_size:
    type: integer
    default: 32
    description: Batch size for the dataset
outputs:
  preprocessed_data:
    type: uri_folder
    description: Folder to save the preprocessed data
command: >
  python preprocessing.py --train_data_path ${{inputs.train_data_path}} --val_data_path ${{inputs.val_data_path}}
  --label_decoder_path ${{inputs.label_decoder_path}} --output_path ${{outputs.preprocessed_data}}
  --tokenizer_model ${{inputs.tokenizer_model}} --max_length ${{inputs.max_length}} --batch_size ${{inputs.batch_size}}